# 202407-202408
1. **Are Language Models Actually Useful for Time Series Forecasting**, arxiv 2024
   * Using experiments to show that adding Language pretrained transformer for time series prediction is not helpful
   * Good news is that the performance is not drop, so time series forecasting can be easily intergrated to LLM
   * Bad news is that LLM does not learn sequential dependencies
2. Teaching Transformers Causal Reasoning through Axiomatic Training, arxiv 2024

   * generate some causal data to train language model, and evaluate the generalization ability with length, order etc
   * data example: 
     * input: a causes b, b causes c, c cause d, does a causes d? 
     * Lable Yes
3. Executable Code Actions Elicit Better LLM Agents, arxiv Feb 2024

   * What: using LLM to generate code for conducting complex tasks
   * How: the paper does not introduce many details
4. Position: Levels of AGI for Operationalizing Progress on the Path to AGI, arxiv 2024 Jun
   * Deepmind's view
   * strict brain-based processes and benchmarks are not inherently necessary for AGI.
   * Turning thinks that whether a machine can think is orthogonal to the question of what the machine can do
     * propose that AGI should be defined in terms of capabilities rather than processes
   * An important property is the inclusion of metacognitive capabilities  - learning new abilities
   * Economically Valuable Work from OpenAI
   * accomplish complex, multi-step tasks in the open world
     * 搬家
     * 做会议纪要
5. OpenAGI: When LLM Meets Domain Experts, NeurIPS 2023
   * Who: Yongfeng Zhang from Rutgers University
6. AutoFlow: Automated Workflow Generation for Large Language Model Agents, arxiv 2024 July
7. DSPY: COMPILING DECLARATIVE LANGUAGEMODEL CALLS INTO SELF-IMPROVING PIPELINES, ICLR 2024 Spotlight
8. AutoGPT: 
9. AgentGPT: 
10. PhyBench: A Physical Commonsense Benchmark for Evaluating Text-to-Image Models, arxiv 2024 July
    * What:
    * Who:
    * How:
    * Note:
11. Pandora: Towards General World Model with Natural Language Actions and Video States
12. An Image is Worth More Than 16 ×16 Patches: Exploring Transformers on Individual Pixels
13. An lmage is Worth 32 Tokens for Reconstruction and Generation
14. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI
15. Solution for SMART-101 Challenge of CVPR Multi-modal Algorithmic Reasoning Task 2024
16. Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows
17. ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models
18. Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion
19. Q-Sparse: All Large Language Models can be Fully Sparsely-Activated
20. Distribution Shift:

    * https://neptune.ai/blog/concept-drift-best-practices
      * types:
        * covariate shift: Shift in the independent variables
        * concept drift: Shift in the relationship between the independent and the target variable
          * sudden drift
          * gradual drift
          * recurrent drift
        * prior probability shift: Shift in the target variable
      * Methods:
        * online learning
        * Periodically re-train
        * Periodically re-train on a representative subsample
        * Ensemble learning with model weighting
        * Feature dropping
      * Reasons:
        * sample selection bias
        * Non-stationary environment
      * Scenarios:
        * Climate changes
        * Pandemic happens
        * Economic Crisis
      * Applications:
        * Spam filter: how to identify new spam types?
        * Time series forecasting
        * Recommendation system
21. PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large Multimodal Models, arxiv Feb 2024
22. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI, CVPR 2024 Oral
23. Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis, arxiv May 2024
24. **Aurora: A Foundation Model of the Atmosphere**, arxiv 2024 - Microsoft

       * What：
       * Why：
       * How:
       * Notes:
25. On the Foundations of Earth and Climate Foundation Models, arxiv May 2024 by TUM
    * What
26. AI Foundation Models for Weather and Climate: Applications, Design, and Implementation, arxiv Sep 2023
27. **ACE Metric: Advection and Convection Evaluation for Accurate Weather Forecasting**, arxiv June 2024 
28. COUPLED OCEAN-ATMOSPHERE DYNAMICS IN A MACHINE LEARNING EARTH SYSTEM MODEL, arxiv June 2024
29. MetaEarth: A Generative Foundation Model for Global-scale Remote Sensing Image Generation, arxiv May 2024 
30. GEO-Bench: Toward Foundation Models for Earth Monitoring, arxiv Dec 2023 
31. Can Multimodal Large Language Models Understand Spatial Relations
32. **Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding**, ICML 2024
33. **Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs**, CVPR 2024
34. **Humanoid Locomotion as Next token Prediction**
35. **Foundation Models for Generalist Geospatial Artificial Intelligence**, arxiv 2023 - NASA
36. **On the Foundations of Earth and Climate Foundation Models**, arxiv 2024 - TUM
    * What
    * Why
    * How
    * Notes: Zhitong Xiong has many earth foundation model works
37. **Data Assimilation with Machine Learning Surrogate Models: A Case Study with FourCastNet**, arxiv 2024
38. **Big data in Earth Science: Emerging Practice and Promise**, Science 2024
39. **SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities**, EMNLP 2023
40. **DO WE REALLY NEED COMPLICATED MODEL ARCHI-TECTURES FOR TEMPORAL NETWORKS**, ICLR 2023 top 5%
41. **SGFormer: Simplifying and Empowering Transformers for Large-Graph Representations**, NeurIPS 2024
    * What:
    * Why:
    * How:
    * Notes
42. **UniTS: Building a Unified Time Series Model**, 

    * What:
    * Why:
    * How:
    * Notes
43. **Unified Training of Universal Time Series Forecasting Transformers**
    * What:
    * Why:
    * How:
    * Notes:
44. **TimeGPT-1**, 

    * What:
    * Why:
    * How:
    * Notes
45. **A decoder-only foundation model for time-series forecasting**, 

    * What:
    * Why:
    * How:
    * Notes
46. **MOMENT: A Family of Open Time-series Foundation Models**, 

    * What:
    * Why:
    * How:
    * Notes
47. **Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting**, 

    * What:
    * Why:
    * How:
    * Notes
48. **WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens**, 

    * What:
    * Why:
    * How:
    * Notes
49. **ViTs for SITS: Vision Transformers for Satellite Image Time Series**, 

    * What:
    * Why:
    * How:
    * Notes
50. **Timer: Transformers for Time Series Analysis at Scale**, 

    * What:
    * Why:
    * How:
    * Notes
51. **xxx**, 

    * What:
    * Why:
    * How:
    * Notes