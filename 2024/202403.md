# 202403-202404
1. **Fusion of ocean data from multiple sources using deep learning: Utilizing sea temperature as an example**, Frontiers in Marine Science, 2023

   * What: propose a deep learning based method to fuse differnt analysis data of the ocean data
     * generate a high quality 3D monthly sea temperature dataset with 0.25 degree

   * Why: 
     * the volume, velocity, variety, and veracity of ocean observation data continue to grow, conventional data assimilation and fusion systems are facing increasingly complicated issues
     * Existing methods for fusing ocean data rely heavily on a prior knowledge of linear principles, normal distributions, and appropriate error covariances.
   * How:
     * Data:
       * 3D data: EN4-analysis, HYCOM, SODA, ECCO, CESM2, BCC-CSM2-HR
       * SST data: ERSST, OISST, HadiSST, AVSIO SLA, and CCMP
     * Use all data of a point as input, process them with a deep learning network and generate the fused data
     * Label: interplorate the insuit observations (EN4-profiles) to 23 layers and 0.25 degree
     * Loss: introduce prior knowledge (e.g., physics) to reduce the spurious high-frequency signal, gurantee the reliability, and improve the interpretability
       * sea temperature gradient constraint: gradient is **difference of sea temperature / difference of depth**
       * sea temperature profile integral constraint: temperature profile integral denotes the sum of all analysis data in one grid point 
   * Notes: 
     * Question: if we used the interplorated data as label, why do we need the fusion??

2. **VisionLLaMA: A Unified LLaMA Interface for Vision Tasks**, arxiv, 2024

   * What: experimentally examine whether the LLaMA architecture can be used for vision 
   * Why: use the same unified architecture and enjoy various deployment techniques designed for LLaMA on the fly
   * Chanllenges:
     * numerous vision tasks rely on pyramid backbones to perform better
     *  necessary to handle input images and videos with different resolutions
   * How:
     * introduce auto-scaled 2D RoPE, which expands rotated positional encoding from 1D to 2D and utilizes interpolation scaling to accommodate arbitrary resolutions
   * Notes

3. **Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teacher**, CVPR 2024

   * What: propose an automatic approach to establish video dataset with high quality captions
   * How:  
     * leveraging multimodal inputs, such as textual video description, subtitles, and individual video frames
     * 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. --> split into video clips --> generate caption ---> select the best caption with a retrieval model ---> 70M videos

4. **YingLong: Skillful High Resolution Regional Short Term Forecasting with Boundary Smoothing**, arXiv 2024

   * What:
   * Why:
   * How:
   * Notes

5. **SGFormer: Simplifying and Empowering Transformers for Large-Graph Representations**, NeurIPS 2023

   * What:
   * Why:
   * How:
   * Notes

6. **UniTS: Building a Unified Time Series Model**, 

   * What:
   * Why:
   * How:
   * Notes

7. **Unified Training of Universal Time Series Forecasting Transformers**

   * What:
   * Why:
   * How:
   * Notes:

8. **TimeGPT-1**, 

   * What:
   * Why:
   * How:
   * Notes

9. **A decoder-only foundation model for time-series forecasting**, 

   * What:
   * Why:
   * How:
   * Notes

10. **MOMENT: A Family of Open Time-series Foundation Models**, 

    * What:
    * Why:
    * How:
    * Notes

11. **Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting**, 

    * What:
    * Why:
    * How:
    * Notes

12. **WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens**, 

    * What:
    * Why:
    * How:
    * Notes

13. **ViTs for SITS: Vision Transformers for Satellite Image Time Series**, 

    * What:
    * Why:
    * How:
    * Notes

14. **Timer: Transformers for Time Series Analysis at Scale**, 

    * What:
    * Why:
    * How:
    * Notes

15. **xxx**, 

    * What:
    * Why:
    * How:
    * Notes