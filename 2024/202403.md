# 202403-202404
1. **Fusion of ocean data from multiple sources using deep learning: Utilizing sea temperature as an example**, Frontiers in Marine Science, 2023

   * What: propose a deep learning based method to fuse differnt analysis data of the ocean data
     * generate a high quality 3D monthly sea temperature dataset with 0.25 degree

   * Why: 
     * the volume, velocity, variety, and veracity of ocean observation data continue to grow, conventional data assimilation and fusion systems are facing increasingly complicated issues
     * Existing methods for fusing ocean data rely heavily on a prior knowledge of linear principles, normal distributions, and appropriate error covariances.
   * How:
     * Data:
       * 3D data: EN4-analysis, HYCOM, SODA, ECCO, CESM2, BCC-CSM2-HR
       * SST data: ERSST, OISST, HadiSST, AVSIO SLA, and CCMP
     * Use all data of a point as input, process them with a deep learning network and generate the fused data
     * Label: interplorate the insuit observations (EN4-profiles) to 23 layers and 0.25 degree
     * Loss: introduce prior knowledge (e.g., physics) to reduce the spurious high-frequency signal, gurantee the reliability, and improve the interpretability
       * sea temperature gradient constraint: gradient is **difference of sea temperature / difference of depth**
       * sea temperature profile integral constraint: temperature profile integral denotes the sum of all analysis data in one grid point 
   * Notes: 
     * Question: if we used the interplorated data as label, why do we need the fusion??
2. **VisionLLaMA: A Unified LLaMA Interface for Vision Tasks**, arxiv, 2024

   * What: experimentally examine whether the LLaMA architecture can be used for vision 
   * Why: use the same unified architecture and enjoy various deployment techniques designed for LLaMA on the fly
   * Chanllenges:
     * numerous vision tasks rely on pyramid backbones to perform better
     *  necessary to handle input images and videos with different resolutions
   * How:
     * introduce auto-scaled 2D RoPE, which expands rotated positional encoding from 1D to 2D and utilizes interpolation scaling to accommodate arbitrary resolutions
   * Notes
3. **Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teacher**, CVPR 2024

   * What: propose an automatic approach to establish video dataset with high quality captions
   * How:  
     * leveraging multimodal inputs, such as textual video description, subtitles, and individual video frames
     * 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. --> split into video clips --> generate caption ---> select the best caption with a retrieval model ---> 70M videos
4. Video generation:
   * Applications:
     * enhance the video quality: resolution, noisy, editing
     * Accessibility: make video creation more accessible to a wider audience
     * personalized and real-time content: generate videos based on users preferences and interactions
     * target marketing, user engagement, interactive experience, education, content creators, 
   * Challenges:
     * data limitations and bias
     * complexity of content, e.g., facial expressions and fine-grained control
     * computation resources for both training and inference
     * Ethical and legal concerns
     * Realism and Uncanny Valley
5. **YingLong: Skillful High Resolution Regional Short Term Forecasting with Boundary Smoothing**, arXiv 2024
   * Institutes: Zhangjiang Institute of Mathematics, 
   * What: 3km resolution regional forecasting based on AI
   * Why:
   * How:
     * utilize the analysis subset of the HighResolution Rapid Refresh (HRRR) dataset, The training dataset ranges from 2015 to 2021, and the data in 2022 is for testing.
     * Network: Swin for local and AFNO for global
     * 12 hours prediction with one-hour interval
     * boundary information: a boundary smoothing is achieved by averaging Yinglong and WRF predicitons during inference around the border
   * Notes
6. **XiHe: A Data-Driven Model for Global Ocean Eddy-Resolving Forecasting**, arxiv, Feb 2024
   * Team: Junqiang Song
   * What: multi-layer ocean forecasting with AI
   * How:
     * land-ocean mask
     * ocean-specific block: linear computational complexity composed by local blocks and global blocks for handling high-resolution data 
     * Prediction: generates the predictions in a non-auto-regressive style, i.e., train a model for each day
     * evaluations are conducted by both satellite observations and in situ observations
       * Multi-layer evaluaitons are also conducted
     * Data: 25 years GLORYS12 reanalysis at 1/12â—¦ horizontal resolution with 50 vertical levels and 10-meter wind from ERA5
       * add satellite SST data into training
7. **AI-GOMS: Large AI-Driven Global Ocean Modeling System**, arxiv, Aug 2023
   * Team: Xiaomeng Huang
   * What: multi-layer ocean forecasting with AI
   * How:
     * generate daily predictions in an auto-regressive manner at 0.25 degree
     * generate higher resolution predictions by AI down-scaling
     * also add a Biochemistry module to predict eight biochemical variables
8. **BUILDING OCEAN CLIMATE EMULATORS**, ICLR 2024 workshop
   * What: explore the role of atmosphere boundary conditions and representations of variables at different time scales for training ocean climate emulators
   * How:
     * Target: velocity and temperature at four regions in the ocean
     * role of boundary: conduct analysis using true atmosphere conditions
     * representations at different scales: training with differents time intervals (e.g., one day, two-day .. ), larger is better
   * Notes: nominal, nascent, mesoscale turbulence, lateral boundary conditions
9. **Uncertainty estimation in satellite precipitation interpolation with machine learning**, arxiv
   * What: quantifying predictive uncertainty for merging satellite and gauge data with machine learning
   * Why:
     * gauges offer high accuracy but are very sparse
     * satellites provide affordable fine-resolution data, but with lower accuracy
   * How:
     * how to do interpolation: 
       * utilizes gauge measurements as the dependent variable and satellite data as predictor variables
       * training a regression algorithm on these paired data points, precipitation can be predicted at any location
     * how to valid: use the monthly guage data and satellite data
       * Gauge: global Historical Climatology Network monthly database, 1 421 locations
       * Satellite: operational PERSIANN, the GPM IMERG
       * Elevation data; The estimation was based on the Amazon Web Services (AWS) Terrain Tiles
     * Baselines: quantile regression, quantile regression forests, generalized random forests, gradient boosting machines, LightGBM, quantile regression neural networks
10. **StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text**, arXiv, March 2024
    * What: generate long videos in an autoregressive manner
      * high motion dynamics
      * good consistency
      * good text alignment
      * good image quality
    * How: utilize long/short-term memory blocks 
      * Baseline: in the *Initialization Stage* the first **16-frame chunk** is synthesized by a text-to-video model
      * Autoregressive generation: feed the last few frames of the previous chunk as condion for further generation
      * Conditional Attention Module (CAM): 
        * leverages short-term information from the last 8 frames of the previous chunk to enable seamless transitions between chunks
        * do not restrict motion by previous shapes and structures
      * Appearance Preservation Module (APM): keep the long-term dependencies of the autoregressive process
        * extracts object or global scene appearance information from an initial image
        * making the autore- gression process robust against loosing object appearance or scene details in the generation
      * Streaming Refinement Stage / Auto-regressive Video Enhancement model: no need for training
        * SDEdit
        * randomized blending
      * reuse pre-trained Video-LDM and train the added modules
        * first train CAM
        * then train CAM+APM: randomly sample an anchor frame out of the first 16 frames
      * Metrics in this paper are useful
    * Notes: stagnation
      * should extract key object information based on the text description
      * keep the scene features
      * FreeNoise: reuse the noisy for all frames
11. Aardvark Weather: end-to-end data-driven weather forecasting, arixv March 2024
    * What: end-to-end global weather forecasting followed by FengWu-Adas
    * How:
      * three modules:
        * map raw data to grided initial state
        * Predictor: make prediction at a 24-hour interval
        * Decoders: postprocess the forecasts for regions or stations
      * how to deal with off-the-grid observations?
        * Use NeuralProcess to handle off-the-grid data
12. Humanoid Locomotion as Next token Prediction
13. SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities, EMNLP 2023
14. **DO WE REALLY NEED COMPLICATED MODEL ARCHI-TECTURES FOR TEMPORAL NETWORKS**, ICLR 2023 top 5%
   * what
   * How
11. **SGFormer: Simplifying and Empowering Transformers for Large-Graph Representations**, NeurIPS 2024
    * What:
    * Why:
    * How:
    * Notes
12. **UniTS: Building a Unified Time Series Model**, 

    * What:
    * Why:
    * How:
    * Notes
13. **Unified Training of Universal Time Series Forecasting Transformers**

    * What:
    * Why:
    * How:
    * Notes:
14. **TimeGPT-1**, 

    * What:
    * Why:
    * How:
    * Notes
15. **A decoder-only foundation model for time-series forecasting**, 

    * What:
    * Why:
    * How:
    * Notes
16. **MOMENT: A Family of Open Time-series Foundation Models**, 

    * What:
    * Why:
    * How:
    * Notes
17. **Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting**, 

    * What:
    * Why:
    * How:
    * Notes
18. **WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens**, 

    * What:
    * Why:
    * How:
    * Notes
19. **ViTs for SITS: Vision Transformers for Satellite Image Time Series**, 

    * What:
    * Why:
    * How:
    * Notes
20. **Timer: Transformers for Time Series Analysis at Scale**, 

    * What:
    * Why:
    * How:
    * Notes
21. **xxx**, 

    * What:
    * Why:
    * How:
    * Notes